{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "052bf350",
   "metadata": {},
   "source": [
    "## Metdata and Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1581efa",
   "metadata": {},
   "source": [
    " * @ Author: Andreas-Nizar Granitzer\n",
    " * @ Create Time: 2025-11-24 22:56:46\n",
    " * @ Description: This notebook documents the model deployment to Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64055ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load packages\n",
    "from transformers import BertForSequenceClassification, BertTokenizer  # tools for NLP with transformers\n",
    "import torch  # for tensor computation and deep learning\n",
    "import json  # for json files\n",
    "import pandas as pd\n",
    "from huggingface_hub import login, create_repo, upload_file, upload_folder, logout\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff47234",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfaaa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file (TODO: ensure HF_TOKEN is set there)\n",
    "load_dotenv()\n",
    "\n",
    "# Get Hugging Face token from environment variables\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# Log in to Hugging Face using the token\n",
    "login(token=hf_token)\n",
    "\n",
    "print(\"âœ… Successfully logged into Hugging Face!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e4d2bc",
   "metadata": {},
   "source": [
    "## Create repo on Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3e329c",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = \"asjc-classification/scibert_multilabel_asjc_classifier\"\n",
    "create_repo(repo_name, \n",
    "            private=True,   # Set to False if you want a public repo\n",
    "            exist_ok=True,  # Avoid error if repo already exists\n",
    "            token=hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c185d682",
   "metadata": {},
   "source": [
    "## Convert model to compatible format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6800d353",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Define the number of labels\n",
    "num_labels = 307 \n",
    "\n",
    "# Load the fine-tuned model weights\n",
    "model_path = \"scibert_multilabel_asjc_classifier.pth\"   # Path to your fine-tuned model weights (not in github repository)\n",
    "checkpoint = torch.load(model_path, map_location=torch.device(\"cpu\"))\n",
    "\n",
    "# Check if it's wrapped in a dictionary\n",
    "if \"model_state_dict\" in checkpoint:\n",
    "    state_dict = checkpoint[\"model_state_dict\"]\n",
    "else:\n",
    "    state_dict = checkpoint  # Directly assign if already a state_dict\n",
    "\n",
    "# Load SciBERT base model with correct number of labels and pass fine-tuned weights\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'allenai/scibert_scivocab_uncased', \n",
    "    num_labels=num_labels,\n",
    "    state_dict=state_dict  # Pass fine-tuned weights\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3623e1b",
   "metadata": {},
   "source": [
    "## Store model and tokenizer on local repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b78af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define save directory\n",
    "SAVE_DIRECTORY = \"scibert_multilabel_asjc_classifier\"\n",
    "\n",
    "# Save fine-tuned model\n",
    "model.save_pretrained(SAVE_DIRECTORY)\n",
    "\n",
    "# Load & save tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\", \n",
    "                                          do_lower_case=True)\n",
    "tokenizer.save_pretrained(SAVE_DIRECTORY)\n",
    "\n",
    "print(f\"Model saved to {SAVE_DIRECTORY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8cb7e3",
   "metadata": {},
   "source": [
    "## Generate label mappings and modify config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402258ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input CSV\n",
    "csv_file = \"Categories.csv\"  # adjust if needed\n",
    "\n",
    "# Paths for output files\n",
    "labels_file = Path(SAVE_DIRECTORY) / \"labels.json\"\n",
    "mappings_file = Path(SAVE_DIRECTORY) / \"label_mappings.json\"\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(csv_file, sep=\";\")\n",
    "labels = df[\"SUBJECT TERM\"].tolist()\n",
    "\n",
    "# Create mappings\n",
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "# Save labels.json\n",
    "with open(labels_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(labels, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Save label_mappings.json\n",
    "with open(mappings_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"label2id\": label2id, \"id2label\": id2label}, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Labels and mappings saved in {SAVE_DIRECTORY}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbba298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the save directory\n",
    "config_path = Path(SAVE_DIRECTORY) / \"config.json\"\n",
    "label_mappings_path = Path(SAVE_DIRECTORY) / \"label_mappings.json\"\n",
    "\n",
    "# Load existing config\n",
    "with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Load label_mappings.json\n",
    "with open(label_mappings_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    label_mappings = json.load(f)\n",
    "\n",
    "# Replace label2id in config\n",
    "config[\"label2id\"] = label_mappings[\"label2id\"]\n",
    "\n",
    "# Ensure id2label remains consistent with label2id\n",
    "config[\"id2label\"] = {str(v): k for k, v in label_mappings[\"label2id\"].items()}\n",
    "\n",
    "# Add new keys\n",
    "config[\"problem_type\"] = \"multi_label_classification\"   # Specify problem type\n",
    "config[\"threshold\"] = 0.3                               # Threshold for multi-label classification\n",
    "\n",
    "# Save updated config\n",
    "with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"config.json updated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ff846c",
   "metadata": {},
   "source": [
    "## Push local folder to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afd1186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# May be also executed if part of the code is changed (e.g., Model card)\n",
    "upload_folder(\n",
    "    folder_path=\"scibert_multilabel_asjc_classifier\",\n",
    "    repo_id=\"asjc-classification/scibert_multilabel_asjc_classifier\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
